{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Копия блокнота \"Копия блокнота \"Копия блокнота \"lab5.ipynb\"\"\"",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX9p5jfTySS"
      },
      "source": [
        "## Задание 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "Применим полученные навыки и решим задачу анализа тональности отзывов. \n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать параметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bHVODC8RV0C"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pydrive.drive import GoogleDrive\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "downloaded = drive.CreateFile({'id':\"1j-DhO_XD5EqzOqVSR4Wz1kNzhLmpeTkZ\"})   # замените id на id своего файла\n",
        "downloaded.GetContentFile('women-clothing-accessories.csv')\n",
        "sms_spam = pd.read_csv('women-clothing-accessories.csv', delimiter='\\t', usecols=[0, 1])\n",
        "sms_spam = sms_spam.sample(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyYJzUx_L17Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea63767-5d0b-4150-f72f-0ddc70c827d4"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "sBFNqTKKTj8M",
        "outputId": "3241973d-25d2-4409-f54e-852f6da62386"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from copy import copy \n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "\n",
        "pymorphy2_analyzer = MorphAnalyzer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def fit_transform(vectorizer):\n",
        "  return vectorizer, vectorizer.fit_transform(my_x_train)\n",
        "\n",
        "def bayes(vectorizer, vectorizer_x_train):\n",
        "  my_clf = MultinomialNB()\n",
        "  my_clf.fit(vectorizer_x_train, my_y_train)  \n",
        "  vectorizer_x_test = vectorizer.transform(my_x_test)\n",
        "  my_pred = my_clf.predict(vectorizer_x_testв_pred, output_dict=True), vectorizer, my_pred\n",
        "\n",
        "def processing(data):\n",
        "  sentences = []\n",
        "  for sentence in data:\n",
        "    sentence = sentence.lower()\n",
        "    for ch in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n':\n",
        "      sentence = sentence.replace(ch,\"\")\n",
        "    sentence = re.sub(r'\\d', '', sentence)\n",
        "    words = word_tokenize(sentence)\n",
        "    for i in range(len(words)):\n",
        "      words[i] = pymorphy2_analyzer.parse(words[i])[0].normal_form\n",
        "    sentences.append(\" \".join(words))\n",
        "  return sentences\n",
        "\n",
        "sms_spam.review = processing(sms_spam.review)\n",
        "my_x_train, my_x_test, my_y_train, my_y_test = train_test_split(sms_spam.review, sms_spam.sentiment, train_size = 0.7)\n",
        "\n",
        "print('word')\n",
        "maximum = (0,0)\n",
        "for i in range(1, 6):\n",
        "  for j in range(i + 1, 6):\n",
        "    vectorizer = CountVectorizer(ngram_range=(i,j), analyzer=\"word\")\n",
        "    count_vectorizer, count_vectorizer_x_train = fit_transform(vectorizer)\n",
        "    a = bayes(count_vectorizer, count_vectorizer_x_train)\n",
        "    if a[0]['accuracy'] > maximum[0]:\n",
        "      maximum = (a[0]['accuracy'], a)\n",
        "print(\"ngram_range:\", str(maximum[1][1]))\n",
        "print(classification_report(my_y_test, maximum[1][2], output_dict=False))\n",
        "\n",
        "print('char')\n",
        "maximum = (0,0)\n",
        "for i in range(1, 6):\n",
        "  for j in range(i + 1, 6):\n",
        "    vectorizer = CountVectorizer(ngram_range=(i,j), analyzer=\"char\")\n",
        "    count_vectorizer, count_vectorizer_x_train = fit_transform(vectorizer)\n",
        "    a = bayes(count_vectorizer, count_vectorizer_x_train)\n",
        "    if a[0]['accuracy'] > maximum[0]:\n",
        "      maximum = (a[0]['accuracy'], a)\n",
        "print(\"ngram_range:\", str(maximum[1][1]))\n",
        "print(classification_report(my_y_test, maximum[1][2], output_dict=False))\n",
        "\n",
        "print('tfid')\n",
        "maximum = (0,0)\n",
        "for i in range(1, 6):\n",
        "  for j in range(i + 1, 6):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(i,j))\n",
        "    count_vectorizer, count_vectorizer_x_train = fit_transform(vectorizer)\n",
        "    a = bayes(count_vectorizer, count_vectorizer_x_train)\n",
        "    if a[0]['accuracy'] > maximum[0]:\n",
        "      maximum = (a[0]['accuracy'], a)\n",
        "print(\"ngram_range:\", str(maximum[1][1]))\n",
        "print(classification_report(my_y_test, maximum[1][2], output_dict=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "word\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b773b0488729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_vectorizer_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_vectorizer_x_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaximum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mmaximum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b773b0488729>\u001b[0m in \u001b[0;36mbayes\u001b[0;34m(vectorizer, vectorizer_x_train)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mmy_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mvectorizer_x_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_x_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mmy_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer_x_testв_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorizer_x_testв_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Задание 5.2 Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391a7045-5a2e-4966-e87e-3a39b87c42eb"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd9a570-cb10-44c0-8ae3-69f8cc0f3127"
      },
      "source": [
        "sentence = 'mom washed the frame'\n",
        "def first_two_letters(x):\n",
        "  return re.findall(r'\\b\\w{2}', x)\n",
        "\n",
        "first_two_letters(sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mo', 'wa', 'th', 'fr']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1fc5f7-2c68-403b-cd37-9efadb7cb5b1"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4793301-da71-4186-c1a5-3ffb19062abe"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d858aa9a-2a8b-4b8b-f9c6-8dab772414f2"
      },
      "source": [
        "result = re.split('\\.', 'Mom washed the frame. Mom washed the frame. Mom washed the frame. Mom washed the frame. \\\n",
        " Mom washed the frame. Mom washed the frame. Mom washed the frame. Mom washed the frame.', maxsplit=3)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mom washed the frame', ' Mom washed the frame', ' Mom washed the frame', ' Mom washed the frame.  Mom washed the frame. Mom washed the frame. Mom washed the frame. Mom washed the frame.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d528a2-37f3-4325-a967-ae325b1f5033"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbcbbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea1aa9c-6233-414b-8bd9-59836cd8d117"
      },
      "source": [
        "result = re.sub(r'\\d', 'DIG', 'I\\'ve bought 3 apples and 75 bananas')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I've bought DIG apples and DIGDIG bananas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c4b53a-f57b-4e57-d720-1bddf95bb02a"
      },
      "source": [
        "result = re.sub(r'http[s]?://([^ ])*', '', 'The best site is https://piazza.com/class/kt725hi4odv52p?cid=50 and i like it')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best site is  and i like it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f25ec4-70b7-493d-e30a-1d240c12a1c6"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07d6f2e-e45f-471e-9b27-86aaa1fb283c"
      },
      "source": [
        "prog = re.compile('[А-Яа-яё\\-]{4,}')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'больше', 'больше', 'слов', 'Что-то']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehZlkvjqr_X7",
        "outputId": "977f9df2-b7cf-4852-cd69-aa739af7f3b9"
      },
      "source": [
        "domens = re.compile('@[A-Za-z0-9]*.[A-Za-z]{2,4}')\n",
        "domens.findall(\"abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}